---
title: "Section 8 Code"
author: "150B/355B Introduction to Machine Learning"
date: "3/2/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this unit we'll continue analyzing a series of press releases from Jeff Flake while he was a House member from Arizona. 

## Section 1: K-means clustering

### 1.1.1 

Download and install the following packages to get started:

```{r}
setwd('~/Dropbox (IPL)/150B Machine Learning/08_Section')
#setwd('YOUR WORKING DIRECTORY')
rm(list=ls())
library(tm)
library(lsa) # install it first, if you haven't already!!
```

### 1.1.2

We already have the files preprocessed and available in `FlakeMatrix.RData`

Load that file to import the object `flake_matrix`.

```{r}
# load file
load('FlakeMatrix.RData')

# print the dimensions of the DTM. How many documents + words are there?
dim(flake_matrix)
```

## 1.2 Runing k Means

### 1.2.1 

We're going to use the function `kmeans` to apply k means clustering.  Read the help file to get a sense for how we apply the model. What are the main inputs? What are the main outputs?

```{r}
?kmeans
```

### 1.2.2 

To use `kmeans`, we're going to work with a **normalized** version of our documents, where we divide every value by the Euclidean length of each document. The Euclidean length of a document X is given by `sqrt(sum(x^2))`.

```{r}
euclid.lengths <- sqrt(rowSums(flake_matrix^2)) # What does this line output?
flake_norm <- flake_matrix/euclid.lengths
```

### 1.2.3 

We can now use `flake_norm` within `kmeans` to cluster the press releases.  

```{r}
k <- 3 # assign k = 3
set.seed(8675309) # Recall that kmeans depends on the initial starting values.  Setting the seed ensures that your code is replicable.  
k_cluster<- kmeans(flake_norm, centers = k)
```

### 1.2.4

Let's take a look at the cluster assignments by examining `k_cluster$cluster`. Which cluster is `10August2007FLAKE293.txt` assigned to?

```{r}
# get cluster assignments of first 10 documents:
head(k_cluster$cluster, 10)
```

### 1.2.5 

We can access the distribution of the clusters with `k_cluster$size`. Which is the biggest cluster? The smallest?
```{r}
k_cluster$size
```

### 1.2.6  Exercise 1

Now try running `kmeans` twice with 3 clusters, but don't set the seed.  what do you notice about the cluster assignments and the number of documents per cluster?

```{r}
# YOUR CODE HERE
```

### 1.2.7

After running `kmeans` several times, run it again with the seed value I provided:

```{r}
k <- 3 # assign k = 3
set.seed(8675309) # Recall that kmeans depends on the initial starting values.  Setting the seed ensures that your code is replicable.  
k_cluster<- kmeans(flake_norm, centers = k)
```

### 1.2.8 

Look at the output for `k_cluster$center`.  Notice that it is a 3 x p matrix, where each column describes the average values of the documents assigned to that cluster. Essentially, each entry is providing the exemplar for the documents assigned to that category.  

```{r}
# assign centers to its own object
centroids <- k_cluster$center

# take a look at the dimensions
dim(centroids)

# What does this output mean?
centroids[, 1000:1005] 
```

## 1.3 K-means interpretation

At this point, we have a partition of the press releases into categories, but we don't have a good sense of what those categories mean.  To interpret those categories, we're going to apply both automatic and manual methods to label the categories. 

### 1.3.1 

Our first approach will be to identify the 10 biggest words for each cluster  I've provided the code here that identifies the ten biggest words associated with each topic.  

```{r}
## First, we're going to create a matrix to store the key words.  
key_words <- matrix(NA, nrow = k, ncol=10)

## Now, we iterate over the clusters 
for(z in 1:k){
	## we want to identify the ten most prevalent words, on average, for the cluster. 
  ## To do that, we can use the k_cluster$centers object to get the cluster centroid.
  ## We then can use the sort function and select the ten most prevalent words.
	ten_most <- sort(k_cluster$center[z,], decreasing=T)[1:10]
	
	## `ten_most` gives us a named vector.
	## Since we're just interested in the top words, we grab the names of this object and store them.
	key_words[z,]<- names(ten_most)
}

key_words[1,]
key_words[2,]
key_words[3,]
```

Based on the keywords, make a guess about the distinct content of each cluster.

### 1.3.2

We might be interested in the words that are prevalent in a cluster but not prevalent elsewhere.

We can modify our keyword procedure slightly to obtain those **distinct** keywords.

Do you notice any differences?

```{r}
key_words2<- matrix(NA, nrow=k, ncol=10)
for(z in 1:k){
	diff<- k_cluster$center[z,] - apply(k_cluster$center[-z, ], 2, mean)
	key_words2[z,]<- names(sort(diff, decreasing=T)[1:10])
}

# Look at the top 10 distinctive words in each cluster
key_words2[1,]
key_words2[2,]
key_words2[3,]
```




## Section 2: Topic modeling
This unit gives a brief overview of the stm (structural topic model) package. Please read the vignette for more detail.

Structural topic model is a way to estimate a topic model that includes document-level meta-data. One can then see how topical prevalence changes according to that meta-data.


## 2.1 Load the ``STM" package and read in the data 

The data we'll be using for this unit consists of all articles about women published in the New York Times and Washington Post, 1980-2014. You worked with a subset of this data in your last homework.

Load the dataset. Notice that we have the text of the articles, along with some metadata.

```{r}
setwd('~/Dropbox (IPL)/150B Machine Learning/08_Section')
#setwd('YOUR WORKING DIRECTORY')
rm(list=ls())
# uncomment if you haven't installed the stm package:
# install.packages("stm") 
library(stm)

# Load Data
women <- read.csv('women-full.csv')
names(women)
```

## 2.2 Preprocessing

### 2.2.1 

STM has its own unique preprocessing functions and procedure, which I've coded below. Notice that we're going to use the `TEXT.NO.NOUN` column, which contains all the text of the articles without proper nouns (which I removed earlier).

```{r}
# Pre-process
temp<-textProcessor(documents = women$TEXT.NO.NOUN, metadata = women)
meta<-temp$meta
vocab<-temp$vocab
docs<-temp$documents

# prep documents in correct format
out <- prepDocuments(docs, vocab, meta)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
```


### 2.2.2 - Exercise 2

Read the help file for the `prepDocuments` function. Alter the code above to keep only words that appear in at least 10 documents.

```{r}
# YOUR CODE HERE
```

## 2.3 Estimate Model

### 2.3.1 

We're now going to estimate a topic model with 15 topics by regressing topical prevalence on region and year covariates. 

Running full model takes a **long** time to finish. For that reason, we're going to add an argument `max.em.its` which sets the number of iterations. By keeping it low (15) we'll see a rough estimate of the topics. You can always go back and estimate the model to convergence.    

```{r eval=FALSE}
# Uncomment to run
model <- stm(docs, vocab, 15, prevalence = ~ REGION + s(YEAR), data = meta, seed = 15, max.em.its = 15)
```

### 2.3.2

Let's see what our model came up with! The following tools can be used to evaluate the model. 

- `labelTopics` gives the top words for each topic. 
- `findThoughts` gives the top documents for each topic (the documents with the highest proportion of each topic)

```{r eval=FALSE}
# Top Words
labelTopics(model)

# Example Docs
findThoughts(model, texts = as.character(meta$TITLE), n=2,topics = 1:15)
```

### 2.3.3 - Exercise 3

Estimate other models using 5 and 40 topics, respectively. Look at the top words for each topic. How do the topics vary when you change the number of topics?

Now look at your neighbor's model. Did you get the same results? Why or why not?

```{r}
# YOUR CODE HERE
```


## 2.4 Interpreting and analyzing the model

### 2.4.1
Let's all load a fully-estimated model that I ran before class.

```{r}
# remove existing objects
rm(list=ls())

# load the already-estimated model.
load("stm.RData")
```

### 2.4.2 - Exercise 4

Using the functions `labelTopics` and `findThoughts`, hand label the 15 topics. Hold these labels as a character vector called `labels`

```{r}
# Store your hand labels below.
# YOUR CODE HERE

labels = c()
```

Now look at your neighbor's labels. Did you get the same results? Why or why not?


### 2.4.3 Analyze topics

We're now going to see how the topics compare in terms of their prevalence across region. What do you notice about the distribution of topic 9? 

```{r}
# Corpus Summary
plot.STM(model, type="summary", custom.labels = labels, main="")

# Estimate Covariate Effects
prep <- estimateEffect(1:15 ~ REGION + s(YEAR), model, meta = meta, uncertainty = "Global", documents=docs)

# plot topic 9 over region
regions = c("Asia", "EECA", "MENA", "Africa", "West", "LA")
plot.estimateEffect(prep, "REGION", method = "pointestimate", topics = 9, printlegend = TRUE, labeltype = "custom", custom.labels = regions, main = "Women's Rights", ci.level = .95, nsims=100)
```

